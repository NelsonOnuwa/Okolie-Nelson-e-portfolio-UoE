<!DOCTYPE HTML>
<!--
	Okolie Nelson University of Essex e-portfolio
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>  e-portfolio Okolie Nelson
	
		</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>e-portfolio</strong> <span>Okolie Nelson</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="landing.html">Landing</a></li>
							<li><a href="generic.html">Generic</a></li>
							<li><a href="elements.html">Elements</a></li>
						</ul>
						<ul class="actions stacked">
							<li><a href="#" class="button primary fit">Get Started</a></li>
							<li><a href="#" class="button fit">Log In</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1>Data, Big Data And Data Wrangling</h1>
									</header>
									<span class="image main"><img src="images/pic11.jpg" alt="" /></span>
									<h1>Data</h1>
									</p>Data is a collection of raw facts and figures that we need to process to extract meaning or information

									<p>Data can be any number (0 to 9), characters (A to Z, a to z), text, words, statements,  
										special characters (*, /, @, #, etc), pictures, sound, or videos that are context less, meaning little or nothing to a human being. 

											<p>For Example: Will Turner, 48, link down, blue, junior, ocean, street

												<p>The data is raw and there is no meaning to it but if we organize this data:

													<p>Will tuner
														<p> H.no - 48, blue ocean,
															<p>Link down street,

																<p>Now, this looks like an Address of the person named Will Turner. Whereas, in the above example it is impossible to make out the meaning of the words.

																	<p>Data is nothing unless it is processed or is aligned in some context. 
																		The data when structured and organized (when the data is processed in some manner) results in Information.

																			<p>Information is “ processed data that is organized, structured or presented in a given context so that the data deliver some logical meaning that may be further utilized in decision making”.

																				<p>Data is something that you can consider as a low level of knowledge where you have some scattered, uncategorized, unorganized entities that do not really mean anything. Whereas Information is the second level of knowledge, where you wire up the data and assign it some context so, the data becomes meaningful. 
																					<h3>Data Types and Data Format</h3>

																					<p>String, Integers, float and boolean are some common examples of data type.
																					
																				<p>There are the four (4) data formats commonly seen in data processing, they include structured, semi structures, quasi-structured and unstructured data format. 
																					
																				<p>Structured data is highly organized, easily accessible and retrievable from where it is stored. It is machine-readable.
																					
																					<p>Semi structured data is partially organized and difficult to retrieve. HTML, XML and other markup languages are examples of semi-structured data.
																					
																						<p>Quasi-structured data involves specifically textual data that has a temporal state and having erratic data formats. Data streams on a social media page or clickstream data from Google searches are examples of quasi-structured data.
																							<p>Unstructured data is not organized, it is difficult to access and to retrieve from where it is stored. Data from social media posts, emails, and chats are examples of unstructured data. Unstructured data could be stored using XML, JSON, or CSV file formats and It is human-readable.
																						
																					
																					
																					
																					


																					<h1>Big data</h1>
																					<p>Big Data defines a situation in which data sets have grown to such enormous sizes that conventional information technologies can no longer effectively handle the size of the data set or the scale and growth of the data set. It has become difficult to manage and even harder to garner value out of it. The primary difficulties are the acquisition, storage, searching, sharing, analytics, and visualization of data. (Ohlhorst, 2012)
																						<p>Big data grows exponentially and is boundless (volume), has varied formats (variety), and has high complexity (voracity and velocity).
																							The four Vs of big data: volume, variety, voracity and velocity

																							<h3>Big Data Security</h3>
																							<p>Many big data tools used by enterprises to identify business opportunities, improve performance, and drive decision-making are open source and not designed with security in mind. This huge increase in data consumption leads to many data security concerns, which include Information theft, distributed denial-of-service (DDoS), ransomware.
																							
																							<h3>Big Data Security Challenges</h3>
																							<p>The constant access by different users, the presence of open source tools and the multiple feeds of data from sources with different protection needs makes it difficult to secure big data. Big data security challenges can be experienced both on-premises and in the cloud. It is prevalent in non-relational database, endpoint vulnerabilities, data mining solutions, access distributed data processing and storage task.
																							
																							<h3>Addressing Big Data Security Threat</h3>
																							<p>Data security threats can he handled using security techniques such as encryption, user access centralized key management and intrusion detection and prevention (IDPS). 
																							One way that organizations can protect data is through encryption, which applies algorithms to scramble data so that it is readable only by someone who holds the key to decrypt it. Encryption takes a piece of data, commonly called the plaintext, combines it with a cryptographic key. This produces a scrambled version of the data called the cipher text. It is possible to decrypt the data to recover the plaintext using the key, but without the key, the cipher text hides all information about the original data, other than its length.
																							<p>With user access control, we turn to cryptographic techniques to secure data in storage. The primary goal of this technique is to enforce access control to data stored in potentially untrusted repositories. That is, we give authorized parties access to the data they need while ensuring that unauthorized parties, either outsiders trying to gain access or malicious insiders in the organization managing the repository, cannot access sensitive data.
																								<p>An intrusion detection and prevention system (IDPS) is a solution that monitors network for simpler threats and then takes flag complex threats to alert security teams to stop any threat detected. 
																							
																							<p>Big data emerges from a variety of sources comprising of Internet of things(IoTs), factories, enterprise resource planning and customer relations management systems according to industry experts. 
										

										<h1>Collaborative Discussion 1</h1>
										<h3>Internet of things (IoTs)</h3>
										<p>Internet of things (IoTs) are physical objects such as our phones, appliances, lighting systems, irrigation systems, security cameras, vehicles and cities equipped with sensors and software, which enable them to interact with each other by collecting and exchanging data via wireless network, with little human intervention. 	
										<p>In the late 1960 and 1970, the inquiry into the idea to connect personal computers (PCs) to other machines began and by 1980s, the invention of local area networks (LANs) provided an effective way to share documents and data across PCs in real time. The invention of the internet by mid 1990s extended LANs capabilities globally and in 1997, a British technologist Kevin Ashton, cofounder of the Auto-ID Center, began working on radio-frequency identification (RFID) technology that would allow physical devices to connect via microchips and wireless signals. During a speech in 1999, Ashton invented the phrase “the Internet of Things”.

										
										
										
											<p>The Internet of things (IoTs) simplifies and automates complicated tasks. It unites the objects under one common infrastructure, it transforms them into smart objects and it remotely controls them.
										
												<p>In transportation, the internet of things (IoTs) technologies ensures the proper tracking and delivery of goods from the manufacturer to the end user in real time. This promotes speed, efficiency and reducing personnel costs. It can also provide a medium for checking the integrity of goods through barcode, the location of goods via GPS, the monitoring of parameters and status variables of the assets via sensors and their transmission via Wi-Fi or GSM/GPRS network. Thus preventing losses, ensuring safety in storage of goods and efficiently locating the products needed.
										
													<p>The collection of useful data about food crops and the detection of potential diseases in advance in agriculture, has led to improved agricultural production processes and the ability to meet food demands.
														<p>In smart cities, the installation of sensors linked with many other devices over the internet gives information to users about malfunctions, electrical failure and issues with traffic jams, energy supply, water shortage, security incidents, etc. In addition, in smart cars the early detection of potential car failure, tire pressure, fueling needs and regular maintenance increases comfort and upgrade driver’s experience.
															<p>The healthcare services can better monitor the heart rate and skin temperature, they can predict different symptoms and prevent potentially life threatening diseases through internet of things (IoTs) technologies.In a global pandemic, fast data collection and diversity is possible.
																<p>With internet of things (IoTs) devices, specific challenges ranging from the inadequacy of security protection leading to loss, stolen, or incorrectly used data to the safety concerns in smart cars and and the electronic waste from IoT products. In addition, the insufficient investigation of long-term effects of IoT technologies and its sustainability are bound to have specific potential drawbacks that need careful consideration.
										
																	
										</p>

									<h1>Data Wrangling</h1>
									</p>To practice high-quality science with data, you need to make sure the data is properly sourced, cleaned, formatted, and pre-processed. 
								</p>After data scientists identify useful data sources for solving the business problem (for instance, in-house database storage or internet or streaming sensor data), they then proceed to extract, clean, and format the necessary data from those sources. 
							</p>	Data wrangling is the process that ensures that the data is in a format that is clean, accurate, formatted to yield a data set that is suitable for exploration and analysis.
						</p>Data wrangling extracts the most valuable information from the dataset.
						</p>The process of data wrangling includes first finding the appropriate data that is necessary for the analysis. This data can be from one or multiple sources, such as tweets, bank transaction statements in a relational database, sensor data, and so on. This data needs to be cleaned. If there is missing data, we will either delete or substitute it, with the help of several techniques. If there are outliers, we need to first detect them and then handle them appropriately. If data is from multiple sources, we will have to perform join operations to combine it.
										
					</p>Generally, the task of data wrangling involves the following steps:
				</p>				1.	Scraping raw data from multiple sources (including web and database tables)
			</p>					2.	Imputing, formatting, and transforming making it ready to for use in the modeling process (such as advanced machine learning)
		</p>						3.	Handling read/write errors
	</p>							4.	Detecting outliers
</p>								5.	Performing quick visualizations (plotting) and basic statistical analysis to judge the quality of your formatted data.
                                         <p>
											<p>
												<p></p>
										<h3>Challenges faced during data wrangling</h3>
										
										<p>The primary challenge of data wrangling stems from the time commitment involved to complete the task and the little amount of automated work that can be carried out on the dataset. Understanding large volume of datasets, the processing of the datasets and combination of different data types possess a significant challenge. </p>
											<p>Using the appropriate tools such as python, data wrangler, google data prep, parse hub, excel and talend can overcome the challenges of data wrangling.
										




												<h1>Data analysis strategy on a three-year period (2000 - 2002) of the dataset (accident level data, Wales)</h1>
												
												<p></p>A data analysis strategy is a plan using data to make informed decisions and measure performance. To design the data analysis strategy on a three-year period (2000 - 2002) of the dataset (accident level data, Wales), I employed the mixed-methods data analysis design and methods because the dataset consisted both of quantitative and qualitative data. The quantitative data analysis design and method enabled me to measure the data in the dataset while the qualitative data analysis design and method enabled me to have an in-depth understanding of how to reduce road accidents in Wales.
												<p></p>I ensured the dataset collected form the online source: https://statswales.gov.wales/Catalogue/Transport/Roads/Road-Accidents/Accident-Level-Data, contained relevant variables for both quantitative and qualitative analysis with the objective to reduce road accidents and the number of casualties thereby improving the safety of road users in Wales. The datasets for each year are in stored in an excel spreadsheet comma separated format (csv) containing forty-nine (49) variables for the three-year period and year 2020 having 2864 values, 2021 having 3288 values and 2022 having 3315 values. The variables consist of both qualitative and quantitative data. The relevant qualitative data (“road type, time, date and day of accident”) provided depth and context. They are descriptive, they answered the "why" or "how" behind the data analysis and may have been collected through study focus groups, interviews, surveys and open-ended questionnaire items. In addition, the relevant quantitative data (“speed limit, total casualties, total vehicles, fatal, serious, slight, older drivers and young drivers”) offered objective and measurable insights, to identify trends, relationships, and make predictions. The quantitative data are numerical, they answered the “what” or “how many” of the data analysis question and may have been collected through experiments, numerical observations and surveys comprising closed-ended questions. By combining both data types, I was able to develop a more comprehensive understanding, leading to more accurate and reliable results. The variables in the dataset have various datatypes namely integers, date, time string and float.
												<p></p>To ensure data integrity, accuracy, and reliability I cleaned and processed the dataset by checking for duplicates, outliers (a data point that significantly differs from the rest of the data in the dataset), missing values, spelling errors and inconsistencies in the dataset. The variables “road” and “Accident Reference” in all the three years dataset had outliers while the variable “road2” in all the three years dataset had missing values. I handled the missing values and duplicates by removing them to ensured consistency in the data types. I also removed irrelevant features and dropped unnecessary columns.
												<p></p>Next, I carried out the data analysis design, which indicated the plan and the expected results of the data analysis project to ensure efficiency and effectiveness. The mixed-methods data analysis design I chose made sure that the data I gathered throughout my data analysis answers the initial question unambiguously. It also created methodological intersections that provided robust, reliable results by organizing the combination of qualitative and quantitative data in a unique manner.
												<p></p>Among the various types of mixed-method data analysis design: the sequential, concurrent, and transformative designs possess distinct advantages. Each design structure organizes the combination of qualitative and quantitative data in a unique manner. I employed the use of the sequential and concurrent designs. The sequential design structure promoted the implementation of the two phases of the data collection, where one form of data enriches or informs the other. It comprises of both the explanatory and exploratory sequential designs, which enabled me to develop a comprehensive understanding with contextual depth. The concurrent design structure enabled the simultaneous collection and independent analysis of my qualitative and quantitative data thereby generating complementary insights. This design produced a well-rounded investigation that widened the lens of understanding through the principle of triangulation. 
												<p></p>The data analysis mixed method I chose played a crucial role by creating a compelling synergy of quantitative and qualitative data analysis method. It harnessed the strengths and compensated for the inherent weaknesses of both qualitative and quantitative data analysis designs thereby ensuring the data analysis outcome are not only statistically significant but also contextually sensitive. It provided a systematic approach to gathering and analyzing the datasets, by ensuring the analysis conducted is rigorous and in a reliable manner providing accurate and valid results, which were essential for drawing meaningful conclusions and making informed decisions from the dataset.
												<p></p>The data analysis mixed method guided the entire data analysis process. It helped formulate my research questions, design the data analysis, analyzed the findings, and interpreted the results. It has the potential to produce well-rounded and credible insights into complex issues. The inherent iterative (repeating the process until perfection) nature of the mixed methods data analysis allowed me to re-assess and re-evaluate the data analysis questions, designs, and interpretations. Without a well-defined data analysis method, the data analysis can become disorganized and lack direction, leading to unreliable and inconclusive outcomes.<p></p>
												c:\Users\User\OneDrive\Documents\PGD-DP\PGD DP RESUBMISSION\RESUBMISSION - Python program file.py

												<h1>Data representation strategy</h1>
												
												<p>A data representation strategy involves selecting the most effective and meaningful ways to visually present data to convey insights. Sixty five percent (65%) of humans are visual learners meaning that they learn from visual cues, 30% from audio and 5% from experience (Bradford, 2004). 
													<p>To represent the data in a three-year period (2000 - 2002) of the dataset (accident level data, Wales), I employed the use of data visualization; a technique of presenting data graphically through use of common graphics, such as charts, plots, and infographics. It enabled me to identify of new trends and pattern in a more efficient way that facilitates the understanding of the dataset. 
														<p>I used the clustered bar chat as my choice of data representation to compare the variables in each year and across the different years. I used the clustered bar chart to compare the total casualties under a particular year and in the different years at different speed limits. It has a title “number of casualties versus speed limits” and showed relationship and major changes of the total casualties with the help of two axes. The x-axis contained the different speed limits for each year, and the y-axis contained the total number of casualties. The height of the horizontal rectangular bars with equal width and space between them is equivalent to the total number of causalities.
															<p>From my clustered bar chart drawn, I could easily visualize and advise in the three-year period (2000 - 2002) of the dataset which of the year had more causalities and which had less causalities at a particular speed limit. 
																<p>With all the benefits of my clustered bar chat, it is limited to monitoring progress and not  forward planning and it is not able to show all the variables in the dataset at once.<p></p>







										<h1>Development Team Project: Project Report</h1>
										
											<h>Evans Cycle Database Proposal</h3>
										</p>To design our Evans cycle logical database, we would employ both logical and physical data
											modelling. Our logical data modelling is concerned with the "what", without paying attention
											to specific functions and capabilities of the database management system (DBMS) that will
											store the data. Here we documented the comprehensive business information requirements
											in an accurate and consistent format. Our process of data logical modelling acknowledges
											that Evans business data is a vital asset we ought to understand and carefully manage.
											(Sloan , 2004)
										</p>In our logical database for Evans cycle, we represented the following business facts in its
											data model: customers, orders, staff members, stores, order items, categories, products,
											stocks, and brands. To perform the logical data-modelling design, we began by defining the
											entities. For example, we defined an entity for all customers because we ought to store
											information about Evans Cycle’s customers. We also define an entity, called orders to store
											detailed order information. Regarding data entities, Attributes are the specific pieces of
											information stored within an entity.
										</p>Next, we defined the primary keys, which is the unique identifier for the entities. In the case
											of “customers” and “order” entity, we chose a unique customer id: “customer_id” and a
											unique order id “order_id” as the primary key. We then used the primary key to define
											separate entities for different types of relationships, which can be one-to-many, many-to-one,
											one-to-one, or many-to-many. We also decided what values such as domains, null values,
											and default values are acceptable for the various attributes of the Evans cycle entity. Finally,
											we normalize the entities to assign the logically related attributes into tables, to avoid update
											anomalies and minimize redundancy.
										</p>In the physical design, we transformed the entities into tables, the instances into rows, and
											the attributes into columns. After which we implemented the physical design by defining the
											various objects and enforcing the constraints on the data relationships. (Fong et al., 2008)
											Data capture is crucial in database management and employing various methods to ensure
											the inclusion of data entities in a database. HR records are usually stored in files and payroll
											systems, captured through dedicated HR processes. Note that discrepancies can result from
											human errors or incomplete data. Additionally, in-store forms and online sign-ups will serve
											as the primary sources for customer data. Data capture will occur through these channels,
											but similar to HR data, maintaining data quality is essential (VisionX, 2023).
											Orders and order items are primarily sourced from both ‘Point of Sale (POS) systems’ and
											e-commerce ‘websites’. This data is captured automatically from the POS system and via
											APIs from online platforms, enabling integration into our database. Store data originates
											from staff input and external records, with ‘manual data entry’ and data file imports used for
											capturing store details (VisionX, 2023). Suppliers provide data, staff data entry and data feed
											files will be the primary source of data from Products, Stocks, Brands and categories. It's
											captured through data feeds for supplier-provided data and manual data entry for product
											details and updates. With the latter being prone to human error which can lead to incomplete
											data (VisionX, 2023).
										</p>The data collected is from Evans Cycle that sells bikes and was obtained from multiple
											different source and consists of 9 tables varying from product information, customer details,
											their stores and employees stored in CSV (Myrick, 2023). This needs to be imported to MS
											SQL (SQL) server but as it currently stands the data is not readable as it is a CSV file and
											this is why we can use Python first to clean this data utilising the package Pandas (Agarwal,
											2023).
										</p>Once imported into Python we convert it to a data frame and will check a few things to clean
											the data: first we will remove any columns that are not necessary for the table which in this
											case was already complete; we will then check for any missing values in the data for
											example in this case some customers didn’t provide a phone number and would mark these
											as NULL; data types in each column need checking and converting them to be readable in
											SQL in this case we would want to convert the prices of products to a float instead of
											integers as these need to be precise (Agarwal, 2023).
											After cleaning we are ready to import our data to SQL and this will be done by converting our
											data frame to tuples, as it is ordered and immutable, then export as a txt file and copy this
											data and insert all of these values.
										</p>The cycling industry is experiencing significant growth, and Evans Cycle's success will
											depend on their ability to adapt to changing demands (Market Size Forecast, 2023). As their
											customer base and transaction volumes increase over time, the adaptability of MS SQL
											Server for upgrading server hardware will be a valuable asset (Bernard, 2023).
											SQL Server is renowned for its powerful query optimisation capabilities. (Sener, 2023). This
											ensures that analysts at Evans Cycle can efficiently retrieve data and allocate more time to
											analytics, facilitating essential business operations and inventory reporting.
											The database will accrue a great amount of product listings, customer data, and historic
											orders which means efficient storage management is crucial. SQL Server provides data
											compression options that help reduce storage space requirements. This feature allows us to
											maximise storage efficiency and manage costs (DAssafMSFT, 2023).
										</p>SQL Server offers ‘backup’ and ‘restore’ functionalities that mitigate data loss risks which are
											essential for safeguarding the business’ data. In addition, its 'data encryption,' 'access'
											control, and 'authentication' mechanisms align with the GDPR, ensuring the secure
											processing of sensitive information and transaction records while enabling authorised
											personnel to access specific database objects (GDPR, undated; Chouhan, 2022).
											Third Normal Form (3NF) will be implemented in the database as it helps minimise data
											redundancy and enhances data integrity (Aslan, 2023). In a cycling store database where
											information on products, sales, and suppliers are interlinked, applying 3NF ensures that data
											is organised into separate, related tables, eliminating indirect dependencies (Aslan, 2023).
											SQL Server's seamless integration of normalised structures complements the logical
											arrangement of our data (Helenclu, 2023).
										</p>We've opted for a SQL database due to the ‘structured’ dataset, which includes product
											details, customer information, and transactions. SQL excels in handling structured data,
											ensuring data ‘consistency’. NoSQL databases are better suited for ‘unstructured’ or
											semi-structured data, which doesn't apply to our structured product, orders, and customer
											data. Given our specific needs, the ‘flexibility’ and ‘scalability’ of NoSQL aren't essential,
											making SQL the perfect choice for effective data management (Community, 2023)</p>



											<h1>Project Report  : Peer review (individual)</h1>
										</p><h3>Rating Scale </h3>
									</p>1	- Did not contribute in this way 
								</p>2	- Willing but not very successful 
							</p>3	- Average 
						</p>4	- Above Average 
					</p>5	- Outstanding 	
				 <h3>Evaluation Criteria</h3>	
			</p>      <h3>Team members: </h3>	           
	</p>                                                      1. Patrick McNeely	       	
</p>	Attends group meetings regularly and arrives on time. 	 5		                       	 
</p>	Contributes meaningfully to group discussions. 	         5		                       	 
</p>	Completes group assignments on time. 	                 5		                       	 
</p>	Prepares work in a quality manner. 	                     5		                       	 
											
              
	</p>                                                     2. Hasnain Khalid        	
</p>	Attends group meetings regularly and arrives on time. 	 5		                       	 
</p>	Contributes meaningfully to group discussions. 	         5		                       	 
</p>	Completes group assignments on time. 	                 5		                       	 
</p>	Prepares work in a quality manner. 	                     5		       <p></p>                	 
											




<h1>Implementing our Evans cycle logical database</h1>
</p>Our logical database was implemented using a specific database management system (DBMS). SQL is the database management system I used to build my Evans cycle logical database. SQL is a standard language for accessing (storing and retrieving) and manipulating databases. I started first by setting up the SQL community server in my system to create the database. To create the database I used the CREATE Database statement followed by the name of the database - CREATE DATABASE Evans cycle. After which I assigned data types to each attributes and then proceeded to create the table. I created tables for all the entities (customers, orders, staff, stores and order items) in my Evans cycle logical database using the CREATE TABLE SQL statement. 
</p>To create the table for one of my entity “customers” for example I used CREATE TABLE followed by the entity “customers” a parentheses then the attributes names and finally the data types. 
</p>Next, I added constraints to some of the attributes. These constraints specify rules for the data in the tables and controls what can be inputted into the DBMS. I added a NOT NULL constraint to the attributes first_name, last _name and phone number in my customer and staff table so that the tables will not accept a record where those attributes are set to NULL. This is because customers and staff would require a first name, last name and phone number. I also set email_id to be UNQUE to ensure every record must have a different value for this attribute and set the customer_id to be the PRIMARY KEY to uniquely identify each row in the table.
</p>After adding constraint, I established relationships between the tables by setting up foreign keys. I used the ALTER Table followed by the name of the participant table to update the table then I used the ADD foreign key followed by the attribute to reference the Primary key. Now I have all of my tables created in a fully functioning relational database.
</p>I populated the tables with data by using the INSERT INTO statement with the table name followed by the data I want in parentheses. The syntax is INSERT INTO table VALUES (data I want to insert, separated by commas). I retrieved the data from the database using the SELECT statement. To retrieve all the columns from the database, I used SELECT with an asterisk while to select only some columns I used SELECT statement followed by the names of the columns I want separated by a comma. To delete records from the table I used the DELETE FROM statement followed by the name of the table while to make changes to the record I used the UPDATE statement</p>













										<h2> References </h2>
														<p>Sandro, N (2020) Internet of things. Available at: Internet of Things (IoT): Opportunities, issues and challenges towards a smart and sustainable future - ScienceDirect (Accessed: 15 February 2024).
															<p>Mori, H (2022) IoT technologies in smart environment: security issues and future enhancement. Available at: IoT technologies in smart environment: security issues and future enhancements - PubMed (nih.gov) (Accessed: 16 February 2024).
																<p>Mohammed, T (2019) Smart home based IoT for real time and secure remote health monitoring of triage and priority system using body sensors. Smart Home-based IoT for Real-time and Secure Remote Health Monitoring of Triage and Priority System using Body Sensors: Multi-driven Systematic Review - PubMed (nih.gov) (Accessed: 16 February 2024).
																	<p>Anish, B (2019) Data protection in big data using encryption Big Data - Data Encryption in Big Data | Encryption Consulting(Accessed: 17 February 2024).
																		<p>Gilad, M (2020) Big data security challenges and solutions Big Data Security: Challenges and Solutions - DATAVERSITY (Accessed: 18 February 2024).
																			<p>Steve, O (2021) The future of data encryption The Future of Data Encryption: What You Need to Know Now | FedTech Magazine (Accessed: 18 February 2024).

																				<p>Ariel, H (2015) Cryptography for Big Data Security Cryptography for Big Data Security (mit.edu) (Accessed: 18 February 2024).
																					Agarwal, M. (2023) Pythonic data cleaning with pandas and NumPy, Real Python. Available
																					at: https://realpython.com/python-data-cleaning-numpy-pandas/ (Accessed: 15 December
																					2023).
																					<p></p>Aslan, M. (2023) Database normalization, Medium. Available at:
																					https://medium.com/@murataslan1/database-normalization-e8f1e08c2193 (Accessed: 14
																					December 2023).
																					<p></p>Bernard, A. (2023) Microsoft SQL Server 2022: Here’s what you need to know for a
																					successful 2023, The SHI Resource Hub. Available at:
																					https://blog.shi.com/business-of-it/microsoft-sql-server-2022-heres-what-you-need-to-know-f
																					or-a-successful-2023/ (Accessed: 14 December 2023).
																					<p></p>Chouhan, P. (2022) SQL Server Vulnerabilities and Assessment, Simple Talk. Available at:
																					https://www.red-gate.com/simple-talk/databases/sql-server/security/sql-server-vulnerabilitiesand-assessment/ (Accessed: 14 December 2023).
																					Community, S. (2023) Choosing the best database: SQL VS NOSQL, Medium. Available at:
																					https://medium.com/codex/choosing-the-best-database-sql-vs-nosql-a816a7ec09fc
																					(Accessed: 14 December 2023).
																					<p></p>Fong, J., Shiu, H. and Cheung, D. (2008) ‘A relational–XML data warehouse for data
																					aggregation with SQL and XQuery’, Software: Practice and Experience, 38(11), pp.
																					1183–1213. doi:10.1002/spe.868.
																					<p></p>Global Bicycle Market Size Forecast 2027 (2023) Statista. Available at:
																					https://www.statista.com/statistics/1356736/bicycle-market-forecast-global/ (Accessed: 14
																					December 2023).
																					<p></p>Guide to the General Data Protection Regulation (GDPR) (no date) www.ico.org.uk.
																					Available at:
																					https://ico.org.uk/media/for-organisations/guide-to-data-protection/guide-to-the-general-dataprotection-regulation-gdpr-1-1.pdf (Accessed: 14 December 2023).
								
																					<p></p>Khonje, J (2023) Police recorded road collision. Available at: https://www.gov.wales (Accessed: 1 December 2023).
																					<p></p>Delonix, K (2017) Which are the most common types of car accidents. Available at: https://www.quantumbooks.com (Accessed: 3 December 2023).
																					<p></p>Thorneycroft Solicitors (2021) Road traffic accidents - the common causes in England and Wales. Available at: https://thorneycroftsolicitors.co.uk (Accessed: 6 December 2023).
																					<p></p>Serious Injury Law (2020) 5 major causes of UK road traffic accidents and how to avoid them. Available at: https://www.seriousinjurylaw.co.uk (Accessed: 9 December 2023).
																					<p></p>Yurday, E (2022) Leading causes of car accidents in Great Britain. Available at: https://www.nimblefins.co.uk (Accessed: 10 December 2023).
																					<p></p>Dye. T (2022) Qualitative Data Analysis. Available at: https://getthematic.com (Accessed: 15 December 2023).
																					<p></p>Bhandari, P (2023) What Is Qualitative Research? | Methods & Examples. Available at: 
																					https://www.scribbr.com/methodology/qualitative-research (Accessed: 15 December 2023).
																					<p>	Regoniel, P (2023) Mixed method research. Available at:  https://simplyeducate.me/2023/11/12/mixed-methods-research (Accessed: 19 December 2023).
																						<p>Datascientyst (2023) Pandas-cheat-sheet-data-cleaning. Available at:  https://datascientyst.com/data-cleaning-steps-python-example (Accessed: 2 January 2024).
																							<p>Holtz, Y (2023) Interactive charts with Plotly. Available at: https://python-graph-gallery.com/about/ (Accessed: 10 January 2024).
																								<p>Datascientyst (2024) Exploratory Data Analysis Python and Pandas with Examples. Available at: https://datascientyst.com/exploratory-data-analysis-pandas-examples (Accessed: 10 January 2024).
																									<p>Plotly graphing libraries (2021) Plotly express in python. Available at: https://plotly.com/python/plotly-express (Accessed: 15 January 2024).
																					
																										</p>Dickson, C (2020) Coding and implementing a relational database using MYSQL Coding and Implementing a Relational Database using MySQL | by Craig Dickson | Towards Data Science (Accessed: 22 February 2024).
								
								
								
								
																				</div>
							</section>

